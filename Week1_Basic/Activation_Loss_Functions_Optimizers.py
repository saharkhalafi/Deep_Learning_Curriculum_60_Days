'''
Activation Functions in Neural Networks
Activation functions introduce non-linearity to neural networks. Without them, a neural network (even a deep one) would behave just like a linear regression model.
â€¢	Use ReLU in hidden layers for general-purpose MLPs and CNNs.
â€¢	Use Tanh in RNNs/LSTMs.
â€¢	Use Sigmoid or Softmax at the output layer based on the task.
â€¢	Use GELU, Swish in modern Transformer-based models.


What is a Loss Function?
A loss function tells the model how wrong its prediction was.
It computes the error between:
Ground truth (true label) vs model prediction (Å·) 
This error (loss) is then minimized using optimization techniques.


What is an Optimizer?
An optimizer adjusts the modelâ€™s weights after computing loss, using gradients from backpropagation.
ðŸ’¡ Goal: Reduce the loss by updating weights to minimize the error.
wt+1=wtâˆ’Î·â‹…âˆ‡wL(w)
â€¢	w: weights
â€¢	Î· (eta): learning rate
â€¢	L: loss function
â€¢	âˆ‡wL: gradient of the loss wrt weights

Stochastic Gradient Descent (SGD):
Simple and powerful baseline optimizer.
w=wâˆ’Î·â‹…âˆ‡L
Pros:
â€¢	Simple and memory efficient
Cons:
â€¢	Can get stuck in local minima
â€¢	Sensitive to learning rate

Adam (Adaptive Moment Estimation)
Combines:
â€¢	Momentum (moving average of gradients)
â€¢	RMSprop (scaling gradients by average of squared grads)
Î¸=Î¸âˆ’Î·â‹…vt+Ïµmt
Where:
â€¢	mt: First moment (mean of gradients)
â€¢	vt: Second moment (variance of gradients)
Pros:
â€¢	Works well for most problems
â€¢	Faster convergence
â€¢	Handles sparse gradients

Common combo:
â€¢	Binary classification â†’ Sigmoid + BCELoss + Adam
â€¢	Multi-class classification â†’ CrossEntropy + Adam
â€¢	Regression â†’ MSE + SGD or Adam


'''
