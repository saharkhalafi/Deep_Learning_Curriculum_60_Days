''' 
Deep Learning is a subfield of Machine Learning (ML) that uses algorithms inspired by the structure and function of the brainâ€™s neural networks. It involves training deep neural networks â€” networks with many layers â€” to learn patterns from large amounts of data, especially high-dimensional data like images, text, speech, and more.
ğŸ‘‰ Deep Learning = Feature Learning + Neural Networks + Lots of Data + Big Compute
ğŸ§  Biological Inspiration:
Deep Learning is loosely inspired by how the human brain works. Specifically:
â€¢	Neuron: Basic unit that processes input and gives an output
â€¢	Artificial Neuron (Perceptron): Mathematical function mimicking neurons
â€¢	Neural Network: Collection of layered neurons
Each layer transforms input data to learn higher-level features â€” the deeper the layers, the more abstract they become.
ğŸ—ï¸ Structure of a Deep Neural Network:
A typical neural network consists of:
1.	Input Layer: Raw data (pixels, text tokens, etc.)
2.	Hidden Layers: Each layer transforms its input into a more useful representation
3.	Output Layer: Final prediction (class, text, number)
Each layer has parameters (weights and biases) which are updated during training using backpropagation and gradient descent.
ğŸ’¡ Why Deep Learning Is Powerful:
â€¢	Learns features directly from raw data
â€¢	Can model complex functions with enough layers
â€¢	Great for unstructured data (images, texts)
â€¢	Benefits massively from GPUs/TPUs
â€¢	Scales well with large datasets
'''

import torch
import torch.nn as nn
import torch.optim as optim

# Sample Neural Network
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 16)   # Input layer: 10 features â†’ 16 neurons
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 1)    # Output layer (binary)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return torch.sigmoid(self.fc2(x))

# Dummy data
inputs = torch.randn(5, 10)         # 5 samples, each with 10 features
labels = torch.randint(0, 2, (5, 1)).float()

# Model + Loss + Optimizer
model = SimpleNN()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training Loop (1 Epoch)
for epoch in range(1):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Loss: {loss.item():.4f}")
